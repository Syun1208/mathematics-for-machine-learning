{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVD.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNzWIc/Mi7cXFB53Xhav4ty"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"V_i1FDpLbUnp"},"source":["# Giới thiệu về SVD"]},{"cell_type":"markdown","metadata":{"id":"1t41T6ieEuHr"},"source":["Singular Value Decomposition (SVD) của một ma trận là một phương pháp phân rã ma trận trong đại số tuyến tính - nhằm giảm ma trận xuống các phần cấu thành của nó giúp cho các phép tính ma trận trở nên đơn giản hơn.\n","\n","**Phát biểu về SVD**: Với ma trận $A \\in R^{mxn}$ - là một rectangular matrix với hạng (rank) của nó là $r \\in [0, min(m, n)]$. SVD của A là một phép phân rã theo công thức sau:\n","$$\n","A = U \\sum V^T (1)\n","$$\n","Trong đó:\n","- $U \\in R^{mxm}$, $V \\in R^{nxn}$ là ma trận trực giao.\n","- $\\sum \\in R^{mxn}$ là ma trận đường chéo với $\\sum_{ii} \\geqslant 0$ và $\\sum_{ij}=0$ (với $i \\neq j$)"]},{"cell_type":"markdown","metadata":{"id":"MTSzj6lZK1Mj"},"source":["Dừng lại ở công thức số (1) này một chút. Một câu hỏi đặt ra đó là tại sao các phần tử trên đường chéo chính của ma trận đường chéo ($\\sum$) luôn lớn hơn 0 ($\\sum_{ii} \\geqslant 0$)? Bước này chúng ta sẽ thử chứng minh nó một chút nhé.\n","\n","Từ công thức (1) ta nhân 2 vế với ma trận chuyển vị của chính nó:\n","$$\n","A^TA = (V \\sum^T U^T)U\\sum V^T= V \\sum^T (U^TU)\\sum V^T = V \\sum^T\\sum V^T = V (\\sum^T\\sum) V^T (2)\n","$$\n","\n","Với phép nhân hai ma trận $(\\sum^T\\sum)$ sẽ luôn cho kết quả trên đường chéo chính luôn dương ($\\sigma^2_{i} \\geqslant 0$ với i = 1, 2, ..., n). Ở đây ta có thể xem như V là eigenvectors với eigenvalues chính là các giá trị trên đường chéo chính của ma trận $(\\sum^T\\sum)$. Vì $\\sigma^2_{i}$ (i = 1, 2, ..., n) luôn dương là các eigenvalue của ma trận $A^TA$ nên suy ra được ma trận $A^TA$ luôn là ma trận nửa xác định dương và  $\\sqrt{\\sigma_i}$ (i=1, 2, ..., n) chính  là các eigenvalue của cuả $A$ hay còn được gọi là singular value của A.\n","\n","Tương tự trên ta cũng suy ra được U là eigenvectors với:\n","$$\n","AA^T = U\\sum V^T(V \\sum^T U^T)= U\\sum (V^TV) \\sum^T U^T = U\\sum \\sum^T U^T = U(\\sum \\sum^T) U^T (3)\n","$$\n","\n","Ta cũng có thể gọi mỗi cột của U chính là một *left-singular vector* của A và mỗi hàng của V chính là một *right-singular vector* của A. Theo quy ước, các singular value sẽ được sắp xếp theo thứ tự với $\\sigma_1 \\geqslant \\sigma_2 \\geqslant .... \\geqslant \\sigma_n \\geqslant 0$"]},{"cell_type":"markdown","metadata":{"id":"t2DPdBBfRklC"},"source":["***Note:*** Singular Value Decomposition của một ma trận bất kỳ luôn tồn tại. Bạn có thể xem thêm phần chứng minh tại link [**này**](https://courses.cs.duke.edu//cps111/spring07/notes/12.pdf)"]},{"cell_type":"markdown","metadata":{"id":"fUntS_EZTsaj"},"source":["Quan sát thấy rằng ma trận đường chéo $\\sum \\in R^{mxn}$ là một rectangular matrix và có thể nói rằng ma trận $\\sum$ là một ma trận có cùng kích thước với A. Điều này có nghĩa là $\\sum$ là một ma trận đường chéo chứa các singular value và thêm một phần đệm với các phần tử là 0 (ma trận 0 - zero padding - $0^T$). Để cụ thể hơn ý này ta sẽ xem với 2 trường hợp sau:\n","\n","**Trường hợp m>n:** Các hàng của ma trận $0^T$ sẽ được thêm từ hàng n+1 đến m như sau:\n","\n","$$\n","\\sum = \n","\\left(\\begin{array}\\\n","\\sigma_1 & 0 & 0 \\\\\n","0 & \\ddots & 0 \\\\\n","0 & 0 & \\sigma_n \\\\\n","0 & \\ldots & 0 \\\\\n","\\vdots &  & \\vdots \\\\\n","0 & \\ldots & 0 \\\\\n","\\end{array}\\right) \n","$$\n","\n","**Trường hợp n>m:** Các cột của ma trận $0^T$ sẽ được thêm từ cột m+1 đến n như sau:\n","\n","$$\n","\\sum = \n","\\left(\\begin{array}\\\n","\\sigma_1 & 0 & 0 & 0 & \\ldots & 0\\\\\n","0 & \\ddots & 0 & \\vdots &  & \\vdots \\\\\n","0 & 0 & \\sigma_m & 0 & \\ldots & 0 \\\\\n","\\end{array}\\right) \n","$$\n","\n","**Với trường hợp n=m:** Ta có thể xem như nó rơi vào 1 trong 2 trường hợp trên"]},{"cell_type":"markdown","metadata":{"id":"oIB6-6W6Xo_C"},"source":["Chúng ta trực quan bằng ảnh với SVD qua hình sau:\n","![image](https://machinelearningcoban.com/assets/26_svd/svd.png)\n","\n","Hình được lấy từ [Bài 26: Singular Value Decomposition - machinelearningcoban](https://machinelearningcoban.com/2017/06/07/svd/). Trong đó ma trận đường chéo với các phần tử không âm giảm giần - màu đỏ càng đậm chứng tỏ giá trị càng cao. Các ô màu trắng thể hiện giá trị 0."]},{"cell_type":"markdown","metadata":{"id":"WezR3pTjYytz"},"source":["# Cách hoạt động"]},{"cell_type":"markdown","metadata":{"id":"lQGosm_rAFOw"},"source":["![image](https://1.bp.blogspot.com/-HIkni_UDF7s/X4QyBtleV-I/AAAAAAAAAKM/nR9LFAFWh7EOTP7GY9h-Y1muA1HyPNYBQCLcBGAsYHQ/s960/Articles%2BPresentation%25285%2529.jpg)"]},{"cell_type":"markdown","metadata":{"id":"elceFmFlbecx"},"source":["Có thể xem A là một phép biến đổi tuyến tính lấy một vector $v_i$ trong không gian dòng của nó thành một vector $u_i=Av_i$ trong không gian cột của nó. SVD phát sinh từ việc tìm một cơ sở trực giao cho không gian dòng được chuyển thành cơ sở trực giao cho không gian cột: \n","$$\n","Av_i=\\sigma_iu_i\n","$$\n","\n","Tương tự với việc tìm cơ sở trực giao cho không gian cột chuyển thành cơ sở trực giao cho không gian dòng. \n","\n","Tổng quát lại ta được\n","$$\n","A \n","\\left(\\begin{array}\\\n","v_1 & ... & v_n \\\\\n","\\end{array}\\right) =\n","\\left(\\begin{array}\\\n","u_1 & ... & u_n \\\\\n","\\end{array}\\right)\n","\\left(\\begin{array}\\\n","\\sigma_1 & ... & 0 \\\\\n","0 & \\ddots & 0 \\\\\n","0 & ... & \\sigma_n \\\\\n","\\end{array}\\right)\n","$$\n","\n","Ta có thể viết ngắn gọn lại như sau:\n","$$\n","AV = U\\sum (4)\n","$$\n","\n","Nhân hai vế với $V^T$:\n","$$\n","A = U\\sum V^T = (1)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"GTN-4LFAdlaB"},"source":["Bây giờ chúng ta sẽ thử chứng minh cách hoạt động của SVD với việc tìm cơ sở trực giao cho không gian cột chuyển thành cơ sở trực giao cho không gian dòng qua ví dụ tính tích của hai vector bất kỳ trong U có bằng 0 hay không qua ví dụ sau: Chứng minh $u_i^T u_j = 0$\n","\n","Từ (1) ta có:\n","$$\n","U = \\frac{AV}{\\sum}\n","$$\n","\n","Xét i=1 và j=2 ta có:\n","$$\n","(\\frac{Av_1}{\\sigma_1})^T(\\frac{Av_2}{\\sigma_2}) = \\frac{v_1^TA^TAv_2}{\\sigma_1\\sigma_2} (5)\n","$$\n","\n","Vì $v_2$ là eigenvector của $A^TA$ nên $\\sigma_2^2$ sẽ là eigenvalue tương ứng của $v_2$ với ma trận $A^TA$. Nên (5) sẽ trở thành:\n","\n","$$\n","\\frac{v_1^T \\sigma_2^2 v_2}{\\sigma_1\\sigma_2} = \\frac{\\sigma_2}{\\sigma_1}v_1^Tv_2 (6)\n","$$\n","\n","vì $v_1^T$ và $v_2$ trực giao nên kết quả của (6) sẽ là 0."]},{"cell_type":"markdown","metadata":{"id":"3K3IjmEtoYiu"},"source":["# Tính SVD của ma trận"]},{"cell_type":"markdown","metadata":{"id":"KkGWbWPAofd4"},"source":["Tính SVD của ma trận: \n","\n","$$\n","A = \n","\\left(\\begin{array}\\\n","3 & 4 \\\\\n","0 & 5 \\\\\n","\\end{array}\\right) \n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"OqPa9cwV2jTe"},"source":["**Bước 1:** Tính $A^T$ và $A^TA$.\n","$$\n","A^T = \n","\\left(\\begin{array}\\\n","3 & 0 \\\\\n","4 & 5 \\\\\n","\\end{array}\\right) => A^TA = \n","\\left(\\begin{array}\\\n","3 & 0 \\\\\n","4 & 5 \\\\\n","\\end{array}\\right)\n","\\left(\\begin{array}\\\n","3 & 4 \\\\\n","0 & 5 \\\\\n","\\end{array}\\right) = \n","\\left(\\begin{array}\\\n","9 & 12 \\\\\n","12 & 41 \\\\\n","\\end{array}\\right)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"_nPJUfGv2ejJ"},"source":["**Bước 2:** Tính eigenvalues của $A^TA$ và sắp xếp theo thứ tự giảm dần\n","\n","$$\n","p_{A^TA}(\\lambda) = det(A^TA-\\lambda I) = det(\n","\\left(\\begin{array}\\\n","9 & 12 \\\\\n","12 & 41 \\\\\n","\\end{array}\\right) - \n","\\left(\\begin{array}\\\n","\\lambda & 0  \\\\\n","0 & \\lambda \n","\\end{array}\\right)\n",") = \n","\\begin{vmatrix}\n","9-\\lambda & 12\\\\\n","12 & 41-\\lambda \n","\\end{vmatrix} \n","$$\n","\n","Tiếp theo ta có:\n","$$\n","p(\\lambda)=(9-\\lambda)(41-\\lambda)-12.12 = \\lambda^2-10\\lambda\n","$$\n","\n","Giải phương trình trên ta có 2 nghiệm $\\lambda_1=45$ và $\\lambda_2=5$\n","\n","Vậy eigenvalue của  $A^TA$ là  $\\sigma_1=\\sqrt{45}$ và $\\sigma_2=\\sqrt{5}$"]},{"cell_type":"markdown","metadata":{"id":"a22L8Dfa2pUo"},"source":["**Bước 3:** Xây dựng ma trận đường chéo $\\sum$ bằng cách đặt các singular value theo thứ tự giảm dần dọc theo đường chéo của nó. Tính nghịch đảo của ma trận đường chéo.\n","\n","$$\n","S = \n","\\left(\\begin{array}\\\n","\\sqrt{45} & 0  \\\\\n","0 & \\sqrt{5} \n","\\end{array}\\right) = \n","\\left(\\begin{array}\\\n","6.708 & 0  \\\\\n","0 & 2.236 \n","\\end{array}\\right) => S^{-1} = \n","\\left(\\begin{array}\\\n","0.149 & 0  \\\\\n","0 & 0.4472 \n","\\end{array}\\right) \n","$$"]},{"cell_type":"markdown","metadata":{"id":"yICkvaxm2tg_"},"source":["**Buớc 4**: Tính eigenvectors từ eigenvalues ở bước 2\n","$$ \n","\\left(\\begin{array}\\\n","9-\\lambda & 12  \\\\\n","12 & 41-\\lambda \n","\\end{array}\\right) x=0\n","$$\n","Xét $\\lambda=5$ ta có:\n","$$ \n","\\left(\\begin{array}\\\n","4 & 12  \\\\\n","12 & 36 \n","\\end{array}\\right) \n","\\left(\\begin{array}\\\n","x_1  \\\\\n","x_2\n","\\end{array}\\right)=0 <=> 4x_1+12x_2=0<=>x_1=-3, x_2=1\n","$$\n","**Kết luận:** với eigenvalue=5 thì eigenvector sẽ là $\\left(\\begin{array}\\\n","-3  \\\\\n","1\n","\\end{array}\\right)$\n","\n","Xét $\\lambda=45$ ta có:\n","$$ \n","\\left(\\begin{array}\\\n","-36 & 12  \\\\\n","12 & -4 \n","\\end{array}\\right) \n","\\left(\\begin{array}\\\n","x_1  \\\\\n","x_2\n","\\end{array}\\right)=0 <=>\n","\\left(\\begin{array}\\\n","-36 & 12  \\\\\n","0 & 0 \n","\\end{array}\\right) \n","\\left(\\begin{array}\\\n","x_1  \\\\\n","x_2\n","\\end{array}\\right)=0<=> -36x_1+12x_2=0<=>x_1=1, x_2=3\n","$$\n","**Kết luận:** với eigenvalue=45 thì eigenvector sẽ là $\\left(\\begin{array}\\\n","1  \\\\\n","3\n","\\end{array}\\right)$ "]},{"cell_type":"markdown","metadata":{"id":"s06qikz_28zn"},"source":["**Bước 5:** tính toán các eigenvector của $A^TA$. Đặt vector riêng này dọc theo các cột của V và tính toán chuyển vị của nó ($V^T$).\n","\n","*Note*: Để các vector này trực giao trong V thì cần chuẩn hóa vector theo L2 norm.\n","\n","**Xét vector ** $\\left(\\begin{array}\\\n","1  \\\\\n","3\n","\\end{array}\\right):$ $L = \\sqrt{x_1^2+x_2^2} = 3.1622$ $=>v_1 = \\left(\\begin{array}\\\n","0.3162  \\\\\n","0.9486\n","\\end{array}\\right)$\n","\n","**Xét vector ** $\\left(\\begin{array}\\\n","-3  \\\\\n","1\n","\\end{array}\\right):$ $L = \\sqrt{x_1^2+x_2^2} = 3.1622$ $=>v_1 = \\left(\\begin{array}\\\n","-0.9486  \\\\\n","0.3162\n","\\end{array}\\right)$\n","\n","Vậy ta có V là:\n","$$\n","\\left(\\begin{array}\\\n","0.3162 & -0.9486  \\\\\n","0.9486 & 0.3162\n","\\end{array}\\right)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"_ArpKJp57s_A"},"source":["**Bước 6:** Tính U với $\n","U = \\frac{AV}{\\sum} = AV\\sum^{-1}\n","$\n","\n","$$\n","U = AV\\sum^{-1} = \n","\\left(\\begin{array}\\\n","3 & 4 \\\\\n","0 & 5 \\\\\n","\\end{array}\\right) \n","\\left(\\begin{array}\\\n","0.3162 & -0.9486  \\\\\n","0.9486 & 0.3162 \\\\\n","\\end{array}\\right)\n","\\left(\\begin{array}\\\n","0.149 & 0  \\\\\n","0 & 0.4472 \n","\\end{array}\\right) = \n","\\left(\\begin{array}\\\n","3 & 4 \\\\\n","0 & 5 \\\\\n","\\end{array}\\right) \n","\\left(\\begin{array}\\\n","0.047 & -0.4242 \\\\\n","0.1413 & 0.1414 \\\\\n","\\end{array}\\right) = \n","\\left(\\begin{array}\\\n","0.7062 & -0.707 \\\\\n","0.7065 & 0.707 \\\\\n","\\end{array}\\right) \n","$$"]},{"cell_type":"markdown","metadata":{"id":"MuGyRecT9aD_"},"source":["Từ việc tính toán SVD ở trên ta thấy rằng SVD sẽ gặp khó khăn khi phải làm việc với số phức và giới hạn của số học dấu chấm dộng có thể khiến một vài ma trận không thể phân tách một cách gọn gàng"]},{"cell_type":"markdown","metadata":{"id":"jM33alQ4ANfe"},"source":["# Thực nghiệm với numpy"]},{"cell_type":"code","metadata":{"id":"nYCfiWzXASbO","executionInfo":{"status":"ok","timestamp":1627789300757,"user_tz":-420,"elapsed":1126,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}}},"source":["import numpy as np \n","from scipy.linalg import svd "],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qlgfXvxgAX3u","executionInfo":{"status":"ok","timestamp":1627789300758,"user_tz":-420,"elapsed":11,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"d2144dea-4d42-4980-bb28-405225554a84"},"source":["A = np.array([\n","      [3, 4],\n","      [0, 5]\n","])\n","print(A)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[[3 4]\n"," [0 5]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tzS75mJiAeAm","executionInfo":{"status":"ok","timestamp":1627789300758,"user_tz":-420,"elapsed":10,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}}},"source":["# SVD\n","U, S, VT= svd(A)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MS0Aoh4eAjR-","executionInfo":{"status":"ok","timestamp":1627789300759,"user_tz":-420,"elapsed":10,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"e20441d5-ff94-49b6-8818-08ea4a7fd820"},"source":["print(U)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[[ 0.70710678 -0.70710678]\n"," [ 0.70710678  0.70710678]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJ8Un2itAkYf","executionInfo":{"status":"ok","timestamp":1627789300759,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"3bcd3373-f2aa-41a7-a668-e8420ec27c2f"},"source":["print(S)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[6.70820393 2.23606798]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Zz4AzTjAtIY","executionInfo":{"status":"ok","timestamp":1627789300759,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"7018a776-7638-4202-c828-2cb3bd284541"},"source":["print(VT)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[[ 0.31622777  0.9486833 ]\n"," [-0.9486833   0.31622777]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RXmK_-PjA3XG","executionInfo":{"status":"ok","timestamp":1627789300760,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"12bc21d2-8c37-4461-a81f-f73557b815bd"},"source":["# reconstruct\n","# create m x n sigma matrix \n","sigma_matrix = np.zeros((A.shape[0], A.shape[1]))\n","\n","# populate sigma with n x n diagonal matrix\n","sigma_matrix[:A.shape[1], :A.shape[1]] = np.diag(S)\n","\n","# reconstruct matrix\n","B = U.dot(sigma_matrix.dot(VT))\n","print(B)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[[3.00000000e+00 4.00000000e+00]\n"," [6.71945856e-16 5.00000000e+00]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sQBFfw0_GYo1"},"source":["# Tổng kết về SVD"]},{"cell_type":"markdown","metadata":{"id":"6sfgTbaPCGou"},"source":["- Ứng dụng phổ biến nhất của SVD chính là giảm chiều dữ liệu.\n","- Dữ liệu có số lượng lớn các feature có thể được giảm xuống một tập hợp nhỏ hơn các tính năng có liên quan nhất đến vấn đề dự đoán.\n","- Kết quả là tạo ra một ma trận có chiều thấp hơn được cho là gần đúng với ma trận gốc. Để thực hiện được điều này, có thể thực hiện SVD trên dữ liệu gốc và chọn ra top k giá trị lớn nhất trong các sigma. Các cột có thể được chọn từ sigma và các dòng được chọn từ $V^T$.\n","- Một ma trận B được tái cấu trúc theo công thức sau: $B = U\\sum k V^T k$ \n"]},{"cell_type":"markdown","metadata":{"id":"__dR2grn2HA4"},"source":["# Xấp xỉ ma trận (Matrix Approximation)"]},{"cell_type":"markdown","metadata":{"id":"BCLOlEFR2X0O"},"source":["Như đã biết SVD là một cách phân rã ma trận $A = U \\sum V^T \\in R^{mxn}$ với $U \\in R^{mxm}$, $V \\in R^{nxn}$ là các ma trận trực giap và $\\sum$ là ma trận chéo hóa chứa các singular value trên đường chéo chính của nó. Thay vì chúng ta làm việc với đầy đủ SVD factorization, thì chúng ta sẽ tìm cách để SVD cho phép biểu diễn một ma trận A dưới dạng một ma trận đơn giản hơn (low-rank) $A_i$, việc tính toán trên ma trận $A_i$ này là đơn giản hơn so với việc tính toán trên toàn bộ SVD.\n","\n","Và ta có một ma trận $A^i \\in R^{mxn}$ với hạng của nó là 1 (rank-1) như sau:\n","$$\n","A_i := \\sigma_iu_iv_i^T\n","$$\n","\n","Rõ ràng với cách biểu diễn trên, ma trận A chỉ phụ thuộc vào i cột đầu tiên của U, V và i giá trị khác 0 trên đường chéo chính của ma trận $\\sum$. Vì vậy ta có thể biểu diễn SVD một cách gọn hơn như sau:\n","$$\n","A = U_i \\sum_i V_i^T\n","$$\n","Với cách biểu diễn này được gọi là *compact SVD*. Trong đó $U_i, V_i$ lần lượt là ma trận được tạo bởi i cột đầu tiên của U và V. $\\sum_i$ là ma trận con được tạo bởi i hàng đầu tiên và i cột đầu tiên của $\\sum$. Với compact SVD nếu ma trận A có rank (r) càng nhỏ hơn số hàng và số cột của nó thì sẽ càng có lợi nhiều về mặt lưu trữ."]},{"cell_type":"markdown","metadata":{"id":"KU_EYjr86Scu"},"source":["![image](https://machinelearningcoban.com/assets/26_svd/img_compress.gif)\n","\n","Hình minh họa cho quá trình xử lý của SVD. Với ảnh đầu vào là một ảnh xám và các ảnh đầu ra với các mức k khác nhau. (Nguồn: [Bài 26: Singular Value Decomposition - machinelearningcoban](https://machinelearningcoban.com/2017/06/07/svd/#-compact-svd)"]},{"cell_type":"markdown","metadata":{"id":"fT0JnX9t66cs"},"source":["Ma trận $A \\in R^{mxn}$ có hạng là r có thể được viết lại dưới dạng tổng của  các ma trận có rank bằng 1 như sau:\n","$$\n","A = \\sigma_1u_1v_1^T + \\sigma_2u_2v_2^T + ... + \\sigma_ru_rv_r^T = \\sum_{i=1}^r\\sigma_iu_iv_i^T (7)\n","$$\n","\n","Nếu thay vì biểu diễn tất cả $A_i$ với i=1, 2, ..., r thì ta có thể biểu diễn $A_i$ với i=1, 2, ..., k. Trong đó k < r và ta được ma trận xấp xỉ với rank-k:\n","$$\n","A_k = \\sum_{i=1}^k\\sigma_iu_iv_i^T (8)\n","$$\n","\n","Xét ví dụ minh họa về sự biểu diễn của ma trận xấp xỉ với k được chọn khác nhau. Ta thấy được rằng, khi chọn một số lượng k đủ tốt thì ma trận xấp xỉ này hoàn toàn có thể thay thế cho ma trận ban đầu khi nó vẫn có thể biểu diễn được một lượng thông tin đầy đủ. Vậy làm thế nào để có thể ước lượng được error của ma trận xấp xỉ với ma trận gốc? Và câu trả lời đó chính là norm - chúng ta có thể sử dụng norm trên vector để ước lượng độ lớn của vector. Tương tự vậy ta hoàn toàn có thể định nghĩa norm cho một ma trận như sau:\n","$$\n","||A||_2 = max_x(\\frac{||Ax||_2}{||x||_2}) = \\sigma_1 (9)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"wili7AcxJ56U"},"source":["Dừng lại ở công thức (9) một chút để cùng chứng minh tại sao *spectral norm* của A lại là $\\sigma_1$? Có thể hiểu  $\\sigma_1$ chính là một sigular value lớn nhất.\n","\n","Điểm quan trọng ở đây đó là ma trận đường chéo có thể tìm thấy cơ sở trực chuẩn của eigenvector $e_i$ với: $Ae_i=\\lambda_ie_i$ (I)\n","\n","Từ đó ta có thể viết $x=\\lambda_1e_1 + \\lambda_2e_2 + ... + \\lambda_ne_n=\\sum_{i=1}^n\\lambda_ie_i$. (II)\n","\n","Norm trong không gian $R^n$ được định nghĩa bởi $||x||^2=<x, x>=\\sum x_i^2$ nên ta có $||x||=<\\sum_{i=1}^n\\lambda_ie_i, \\sum_{i=1}^n\\lambda_ie_i>=\\sqrt{\\sum_{i=1}^n\\lambda_i^2e_i^2}=\\sqrt{\\sum_{i=1}^n\\lambda_i^2}$ với $e_i = <e_i, e_j> = 1$.\n","\n","Đặt $B = A^*A$ là một Hermitian matrix. \n","\n","Từ (I) và (II) ta có: $Ax_i = \\sum_{i=1}^n\\lambda_ie_i$ Nên $Bx= B(\\sum_{i=1}^n\\lambda_ie_i)= \\sum_{i=1}^n\\lambda_iBe_i = \\sum_{i=1}^nb_i\\lambda_ie_i$ Với $b_{j0}$ là eigenvalue lớn nhất của B.\n","\n","Vì vậy ta có: \n","$$\n","||Ax|| = \\sqrt{<Ax, Ax>} = \\sqrt{<x, A^*Ax>} = \\sqrt{< \\sum_{i=1}^nb_i\\lambda_ie_i,  \\sum_{i=1}^nb_i\\lambda_ie_i>} = \\sqrt{ \\sum_{i=1}^n(b_i\\lambda_ie_i)^2}(10)\n","$$\n","\n","Áp dụng Cauchy–Schwarz (10) trở thành:\n","$$\n","\\sqrt{ \\sum_{i=1}^n(b_i\\lambda_ie_i)^2} \\leqslant max_{1\\leqslant j \\leqslant n}(\\sqrt{b_j}) * ||x|| \n","$$\n","\n","Nếu $||A||=max(\\frac{||Ax||}{||x||})=1$ thì $||A|| \\leqslant  max_{1\\leqslant j \\leqslant n}(\\sqrt{b_j})$ (III)\n","\n","Với $x_0 = e_{j0}$ ta có $||A||^2  \\geqslant <x_0, Bx_0> = <e_{j0}, Be_{j0}> = <e_{j0}, b_{j0}e_{j0}> = b_{j0}$ (IV)\n","\n","Từ (III) và (IV) ta có $||A||=max_{1\\leqslant j \\leqslant n}(\\sqrt{b_j})$ với $b_j$ là eigenvalue của $B=A^*A$\n","\n","Kết luận:  $||A||_2= max_x(\\frac{||Ax||_2}{||x||_2}) = \\sigma_{max}(A)$ với A là ma trận đường chéo với các phần tử trên đường chéo là các singular value đã được sắp xếp theo thứ tự giảm dần nên $\\sigma_{max}(A)=\\sigma_1$.\n","\n","Như vậy (9) đã được chứng minh xong\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1sBwvsYohLky"},"source":["Điều này hoàn toàn tương tự với các chuẩn khác. Các bạn có thể xem thêm chứng minh với Frobineous norm tại [Bài 26: Singular Value Decomposition - machinelearningcoban](https://machinelearningcoban.com/2017/06/07/svd/#-compact-svd)"]},{"cell_type":"markdown","metadata":{"id":"9Gn3VZXehH4a"},"source":["# Xấp xỉ rank k tốt nhất"]},{"cell_type":"markdown","metadata":{"id":"kTHI1ZscYhGT"},"source":["Xét một ma trận $A \\in R^{mxn}$ với rank r và $B \\in R^{mxn}$ với rank k. Với $k \\leqslant r$ ta có $A_k$ chính là nghiệm của bài toán sau đây:\n","$$\n","A_k = min_B||A-B||_2\n","$$"]}]}